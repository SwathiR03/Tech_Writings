Have you ever wondered how much of data you leave behind when you use the internet? Ever wondered
how the ever-growing mountain of data being generated by us is stored and used? In today’s world,
industries, services and even regular users on the internet generate tons of data and this data growth has
been exponential. A study by IDC has shown that from 2009 to 2020, there has been a 44x increase of
data from 0.8 Zettabytes to 35 Zettabytes.[1]

Traditionally, Companies used to store their data in traditional Relational Database Management Systems
like MySQL and run complex “Extract-Transform-Load” (ETL) procedures once a day where they combined
data from multiple sources into a single data warehouse and generated reports from it. However, storing
data in structured databases got harder while the quantity and the variety of data kept increasing with no
signs of slowing down. A change in approach was necessary.

Big Data is a new domain of data architecture, data storage, processing and consumption. However, what
qualifies as “Big” Data? The characteristics of Big Data can be understood from IBM’s 4Vs of Big Data
which are:
1. Volume: The scale of data is huge, from terabytes to petabytes of data.
2. Variety: Data comes in many forms- structured, unstructured, text, multimedia which makes it                                                                           
harder to process.
3. Velocity: Data is in motion at very high speeds and analyses would need to be done in the fraction
of a second to maintain performance.
4. Veracity: Data can have errors or be inconsistent and hence it is essential to manage reliability
and predictability of such data.

With the importance of Big Data established, it is of utmost importance to understand the Big Data
pipeline which includes data gathering, storage, processing and consumption. To leverage the power of
Big Data, it needs to be converted into meaningful information.

Data of such large volume can’t be processed smoothly on a single computer system, regardless of how
powerful the hardware is. Thus, Big Data follows the paradigm of Distributed Computing where the
processing is carried out across multiple computers that run on commodity hardware with each computer
storing multiple replicas of data to prevent data loss. This ensures reliability, scalability and faster
processing times.

The logical layers In the Big Data Pipeline are:

* Layer 1: Identification of sources of data which involves gathering of data from various sources,
identifying data types and their formats.
* Layer 2: Data ingestion and acquisition which involves using of Extract-Transform-Load (ETL) in batches
or real time, structuring the data to add meaning to it and pre-processing data to remove missing values
or inconsistencies.
* Layer 3: Data storage through the use of Hadoop Distributed File System, Traditional Relational Database
Management Systems or Not Only SQL data stores like MongoDB, HBase or Cassandra which do not follow
a fixed schema. The choice of storage will be based on factors like whether the data is historic or
incremental, its format, querying patterns and data consumption.
In case of traditional structured data running into millions such as logs of transactions, the choice of
storage would be Relational Database Management Systems such as Oracle, Sybase or DB2 and processing
would be done through Data warehouses or Data marts.
In case of data which run into terabytes or more and is unstructured like social media data, messages or
emails, Not Only SQL databases like MongoDB, HBase or Cassandra are preferred.
In case of continuous streaming data like Stock Market Data, Social Media Analytics such as likes or clicks,
data is stored in distributed file systems like Hadoop and processed through tools like Spark and Mesos.
* Layer 4: Data processing which involves using techniques like MapReduce and applications like Hive, Pig
or Spark. The processing can be in batches, real-time or a combination of both. Processing can also be
synchronous(sequential) or asynchronous.
* Layer 5: Data consumption which refers to the usage of data. This includes exporting the data to the cloud
or web and using it for downstream tasks such as Machine Learning, Business Intelligence and Data
Analytics.
Big Data has brought a paradigm shift in the way data has been viewed and used in the industry. Various
companies from Big Tech to Political Campaigners are using Big Data to get meaningful insights from data
and achieve their goals. As British Mathematician Clive Humby said, “Data is the new oil”. Big Data thus
holds immense potential in businesses to make informed decisions, operate more efficiently and gain
competitive advantages.
[1]: 6 Predictions About Data In 2020 And the Coming Decade by Gil Press
